"""
Evaluation script for the Hierarchical Transformer Chatbot

This script provides tools for evaluating the performance of the chatbot model
using various metrics:
1. Perplexity - Measures how well the model predicts the test data
2. Response diversity - Evaluates the variety of responses generated
3. Interactive evaluation - Manual testing of the model's responses
4. Automated conversation testing - Simulates conversations

Usage:
    python3 evaluate_model.py --model_path checkpoints/best_model.pt --eval_mode all
"""

import argparse
import json
import os
import time
import numpy as np
import torch
import torch.nn.functional as F
from tqdm import tqdm
import matplotlib.pyplot as plt
from collections import Counter
import random
import logging

# Import from our modules
from hiearchal_transformer import (
    HierarchicalTransformer, tokenize_conversation, generate_response,
    SOS_TOKEN_ID, EOS_TOKEN_ID, SEP_TOKEN_ID, PAD_TOKEN_ID, 
    vocab, word_to_id, id_to_word, collate_fn, ChatDataset
)
from data_loader import fetch_synthetic_data, SyntheticDataGenerator

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("ModelEvaluation")

def load_model(model_path, device=None):
    """
    Load a model from a checkpoint file.
    
    Args:
        model_path: Path to the model checkpoint
        device: Device to load the model on (cuda, mps, cpu)
        
    Returns:
        The loaded model
    """
    if device is None:
        if torch.cuda.is_available():
            device = torch.device("cuda")
        elif torch.backends.mps.is_available():
            device = torch.device("mps")
        else:
            device = torch.device("cpu")
    
    logger.info(f"Loading model from {model_path} to {device}")
    
    # Load checkpoint
    try:
        checkpoint = torch.load(model_path, map_location=device)
        
        # Get model parameters from checkpoint
        epoch = checkpoint.get('epoch', 0)
        loss = checkpoint.get('loss', float('inf'))
        perplexity = checkpoint.get('perplexity', float('inf'))
        
        logger.info(f"Loaded checkpoint from epoch {epoch} with loss {loss:.4f} and perplexity {perplexity:.4f}")
        
        # Create model with same parameters
        model = HierarchicalTransformer(
            vocab_size=len(word_to_id),
            d_model=128,  # Could be stored in checkpoint
            d_ff=512,     # Could be stored in checkpoint
            n_heads=4,    # Could be stored in checkpoint
            n_local_layers=2,  # Could be stored in checkpoint
            n_global_layers=2  # Could be stored in checkpoint
        )
        
        # Load state dict
        model.load_state_dict(checkpoint['model_state_dict'])
        model = model.to(device)
        model.eval()
        
        return model
    except Exception as e:
        logger.error(f"Error loading model: {e}")
        raise

def calculate_perplexity(model, test_data, pad_token_id=PAD_TOKEN_ID, device=None):
    """
    Calculate the perplexity of the model on test data.
    
    Args:
        model: The model to evaluate
        test_data: DataLoader containing test data
        pad_token_id: Token ID for padding
        device: Device to run evaluation on
        
    Returns:
        Perplexity value
    """
    if device is None:
        device = next(model.parameters()).device
        
    model.eval()
    total_loss = 0
    total_tokens = 0
    
    with torch.no_grad():
        for tokens, pad_mask in test_data:
            tokens = tokens.to(device)
            pad_mask = pad_mask.to(device)
            
            logits = model(tokens, pad_mask)
            target = tokens[:, 1:].contiguous()
            logits = logits[:, :-1].contiguous()
            
            loss = F.cross_entropy(
                logits.view(-1, logits.size(-1)),
                target.view(-1),
                ignore_index=pad_token_id,
                reduction='sum'
            )
            
            total_loss += loss.item()
            total_tokens += (target != pad_token_id).sum().item()
    
    # Handle edge case
    if total_tokens == 0:
        return float('inf')
        
    perplexity = torch.exp(torch.tensor(total_loss / total_tokens))
    return perplexity.item()

def measure_response_diversity(model, test_inputs, device=None, temperature=1.0, num_responses=10):
    """
    Measure the diversity of responses generated by the model for a set of test inputs.
    
    Args:
        model: The model to evaluate
        test_inputs: List of input strings to test
        device: Device to run evaluation on
        temperature: Temperature for sampling
        num_responses: Number of responses to generate per input
        
    Returns:
        Dictionary of diversity metrics
    """
    if device is None:
        device = next(model.parameters()).device
    
    model.eval()
    
    # Metrics to track
    total_unique_tokens = 0
    total_tokens = 0
    unique_responses = 0
    responses_per_input = []
    
    for input_text in tqdm(test_inputs, desc="Measuring diversity"):
        # Generate multiple responses for the same input
        responses = []
        for _ in range(num_responses):
            resp = generate_response(
                model, input_text, word_to_id, id_to_word,
                SOS_TOKEN_ID, EOS_TOKEN_ID, SEP_TOKEN_ID,
                device=device, temperature=temperature
            )
            responses.append(resp)
        
        # Count unique responses
        unique_resp = len(set(responses))
        unique_responses += unique_resp
        responses_per_input.append(unique_resp)
        
        # Count token diversity
        all_tokens = []
        for resp in responses:
            tokens = resp.split()
            all_tokens.extend(tokens)
            total_tokens += len(tokens)
        
        # Count unique tokens
        token_set = set(all_tokens)
        total_unique_tokens += len(token_set)
    
    # Calculate metrics
    metrics = {
        "unique_response_ratio": unique_responses / (len(test_inputs) * num_responses),
        "token_diversity_ratio": total_unique_tokens / max(1, total_tokens),
        "avg_unique_responses_per_input": sum(responses_per_input) / len(test_inputs)
    }
    
    return metrics

def run_automated_conversation(model, device=None, turns=5, temperature=0.8):
    """
    Run an automated conversation by having the model talk to itself.
    
    Args:
        model: The model to evaluate
        device: Device to run on
        turns: Number of conversation turns
        temperature: Temperature for sampling
        
    Returns:
        The conversation history
    """
    if device is None:
        device = next(model.parameters()).device
    
    model.eval()
    
    # Start with a greeting
    conversation = ["Hello, how are you today?"]
    
    for i in range(turns):
        # Generate response to the last message
        response = generate_response(
            model, conversation[-1], word_to_id, id_to_word,
            SOS_TOKEN_ID, EOS_TOKEN_ID, SEP_TOKEN_ID,
            device=device, temperature=temperature
        )
        
        # Add response to conversation
        conversation.append(response)
        
        # Generate next prompt
        prompt = generate_response(
            model, response, word_to_id, id_to_word,
            SOS_TOKEN_ID, EOS_TOKEN_ID, SEP_TOKEN_ID,
            device=device, temperature=temperature
        )
        
        # Add prompt to conversation
        conversation.append(prompt)
    
    return conversation

def interactive_evaluation(model, device=None, temperature=0.8):
    """
    Interactive evaluation where the user can chat with the model.
    
    Args:
        model: The model to evaluate
        device: Device to run evaluation on
        temperature: Temperature for sampling
    """
    if device is None:
        device = next(model.parameters()).device
    
    model.eval()
    
    print("\n" + "="*50)
    print("Interactive Evaluation Mode")
    print("Type 'exit' to end the conversation.")
    print("="*50 + "\n")
    
    conversation_history = []
    
    while True:
        user_input = input("You: ")
        if user_input.lower() == 'exit':
            break
        
        conversation_history.append(user_input)
        
        # Generate response
        start_time = time.time()
        response = generate_response(
            model, user_input, word_to_id, id_to_word,
            SOS_TOKEN_ID, EOS_TOKEN_ID, SEP_TOKEN_ID,
            device=device, temperature=temperature
        )
        end_time = time.time()
        
        conversation_history.append(response)
        
        # Print response with timing information
        print(f"Bot: {response}")
        print(f"(Response generated in {(end_time - start_time):.3f} seconds)")
        print()
    
    return conversation_history

def prepare_test_data(data_path=None, num_samples=500):
    """
    Prepare test data for evaluation.
    
    Args:
        data_path: Path to test data JSON file
        num_samples: Number of synthetic samples to generate if data_path is None
        
    Returns:
        DataLoader with test data
    """
    # Either load from file or generate synthetic
    if data_path and os.path.exists(data_path):
        with open(data_path, 'r') as f:
            conversations = json.load(f)
        logger.info(f"Loaded {len(conversations)} conversations from {data_path}")
    else:
        # Generate synthetic test data
        generator = SyntheticDataGenerator()
        conversations = generator.generate_conversations(num_samples)
        logger.info(f"Generated {len(conversations)} synthetic test conversations")
    
    # Tokenize conversations
    tokenized_conversations = []
    for conv in conversations:
        tokenized_conversations.extend(
            tokenize_conversation(conv, word_to_id, SOS_TOKEN_ID, EOS_TOKEN_ID, SEP_TOKEN_ID)
        )
    
    # Create dataset
    dataset = ChatDataset(tokenized_conversations, max_seq_len=64, pad_token_id=PAD_TOKEN_ID)
    test_loader = torch.utils.data.DataLoader(
        dataset, batch_size=16, shuffle=False, collate_fn=collate_fn
    )
    
    return test_loader

def main(args):
    """Main evaluation function"""
    # Determine device
    device = None
    if torch.cuda.is_available():
        device = torch.device("cuda")
    elif torch.backends.mps.is_available():
        device = torch.device("mps")
    else:
        device = torch.device("cpu")
    
    # Load model
    model = load_model(args.model_path, device)
    
    # Prepare test inputs for diversity measurement
    test_inputs = [
        "Hello, how are you?",
        "What do you think about artificial intelligence?",
        "Tell me a joke",
        "What's the weather like today?",
        "Can you help me with my homework?",
        "What's your favorite movie?",
        "How do I learn programming?",
        "Tell me about yourself",
        "What's the meaning of life?",
        "What can you do?"
    ]
    
    # Run evaluations based on mode
    results = {}
    
    if args.eval_mode in ['perplexity', 'all']:
        logger.info("Calculating perplexity...")
        test_loader = prepare_test_data(args.test_data, args.num_test_samples)
        perplexity = calculate_perplexity(model, test_loader, PAD_TOKEN_ID, device)
        results['perplexity'] = perplexity
        logger.info(f"Model perplexity: {perplexity:.4f}")
    
    if args.eval_mode in ['diversity', 'all']:
        logger.info("Measuring response diversity...")
        diversity_metrics = measure_response_diversity(
            model, test_inputs, device, args.temperature, args.num_responses
        )
        results['diversity'] = diversity_metrics
        logger.info(f"Diversity metrics: {diversity_metrics}")
    
    if args.eval_mode in ['auto_convo', 'all']:
        logger.info("Running automated conversation...")
        conversation = run_automated_conversation(
            model, device, args.conversation_turns, args.temperature
        )
        results['automated_conversation'] = conversation
        
        # Print the conversation
        print("\nAutomated Conversation:")
        for i, message in enumerate(conversation):
            speaker = "Bot 1" if i % 2 == 0 else "Bot 2"
            print(f"{speaker}: {message}")
    
    if args.eval_mode == 'interactive':
        logger.info("Starting interactive evaluation...")
        conversation_history = interactive_evaluation(model, device, args.temperature)
        results['interactive_conversation'] = conversation_history
    
    # Save results if requested
    if args.save_results:
        output_path = args.output_file or f"evaluation_results_{time.strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_path, 'w') as f:
            json.dump(results, f, indent=2)
        logger.info(f"Results saved to {output_path}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Evaluate the hierarchical transformer chatbot")
    
    parser.add_argument('--model_path', type=str, default="checkpoints/best_model.pt",
                        help="Path to the model checkpoint")
    parser.add_argument('--eval_mode', type=str, default='all', 
                        choices=['perplexity', 'diversity', 'interactive', 'auto_convo', 'all'],
                        help="Evaluation mode")
    parser.add_argument('--test_data', type=str, default=None,
                        help="Path to test data JSON file (optional)")
    parser.add_argument('--num_test_samples', type=int, default=500,
                        help="Number of synthetic test samples if no test data is provided")
    parser.add_argument('--temperature', type=float, default=0.8,
                        help="Temperature for sampling")
    parser.add_argument('--num_responses', type=int, default=10,
                        help="Number of responses to generate per input for diversity measurement")
    parser.add_argument('--conversation_turns', type=int, default=5,
                        help="Number of turns in automated conversation")
    parser.add_argument('--save_results', action='store_true',
                        help="Save evaluation results to file")
    parser.add_argument('--output_file', type=str, default=None,
                        help="Output file path for evaluation results")
    
    args = parser.parse_args()
    main(args)
